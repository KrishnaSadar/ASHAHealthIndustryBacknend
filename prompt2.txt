I want a full Node.js backend (JavaScript) that implements the exact data models and endpoints I describe below. Output the complete project: directory tree, package.json, vercel.json, and every source file with working code and clear inline documentation comments. Use Express + Mongoose + MongoDB. All secrets and keys must be read from environment variables (use dotenv). The code should be production-ready (basic validation, error handling, logging) and must not contain logical errors. You may add small, non-breaking stabilizing features (e.g., password hashing, request validation, simple auth, rate limiting, cron job) but do not change the core functionality or data models. At the end include a short README with instructions to run locally and basic Vercel deployment instructions.

Global requirements

Language: JavaScript (Node.js, not TypeScript).

Frameworks/libraries: express, mongoose, dotenv, bcryptjs, jsonwebtoken (optional but include if you implement auth), express-async-handler, joi or express-validator (for request validation), node-cron (for hourly prediction job), cors, helmet, morgan, rate-limit (optional simple rate limiter).

Use async/await, proper error handling middleware, and organized controllers/services.

All secret values must come from environment variables. Document exact env var names.

Provide a vercel.json configured for deploying the Express app as serverless functions on Vercel (use serverless-http if needed). Provide start and build scripts in package.json.

Include a utils/placeholder.js file with three synchronous placeholder functions that the code will call (implementation should be simple and return deterministic dummy values, but structured so they are easy to replace later):

latLngToZone(latitude, longitude) → returns a zone string (taluka).

zoneToWaterParams(zone) → returns { ph, turbidity }.

callMlPredictionApi(payload) → sends payload to an external ML API (read URL from env) and returns whatever JSON body the API returns. For now it may simulate a fetch if PREDICTION_API_URL is not provided.

Data models (Mongoose schemas) — use exactly these fields and types

Villager

{
  _id: ObjectId,           // auto-generated
  name: String,
  lastName: String,
  mobileNo: String,
  latitude: Number,
  longitude: Number,
  zone: String,            // derived from lat/lng using placeholder
  age: Number,
  gender: String
}


PatientRequest (patient complaint)

{
  _id: ObjectId,           // complaint id auto-generated
  userId: ObjectId,        // link to Villager _id
  state: String,
  district: String,
  zone: String,
  symptoms: [String],
  age: Number,
  complaint: String,
  status: Number,          // 1 = unsolved, 2 = pending, 3 = solved
  days: Number,
  ph: Number,
  turbidity: Number,
  createdAt: Date
}


DirtyWaterComplaint

{
  _id: ObjectId,
  userId: ObjectId,
  state: String,
  district: String,
  zone: String,
  age: Number,
  complaint: String,
  status: Number,          // 1 = unsolved, 2 = pending, 3 = solved
  photo: String,           // URL
  ph: Number,
  turbidity: Number,
  createdAt: Date
}


AshaWorker

{
  _id: ObjectId,           // user_id
  passwordHash: String,    // store hashed password
  mobileNo: String,
  name: String,
  lastName: String,
  zone: String,            // allotted zone
  age: Number,
  gender: String,
  status: Number           // 1 = on-duty, 0 = off-duty
}


ZonePrediction — store prediction results per zone

{
  _id: ObjectId,
  zone: String,
  payloadSent: Mixed,       // request sent to ML api
  predictionResponse: Mixed,// raw JSON from ML API
  receivedAt: Date
}


HelpRequest — Ask_for_help entries when worker taps help

{
  _id: ObjectId,
  zone: String,
  workerId: ObjectId,      // optional — if a worker is logged in when pressing help
  createdAt: Date
}


Endpoints (implement exactly these behaviors and routes)

POST /api/v1/villagers — add_villager

Body JSON: { mobile_no, name, last_name, latitude, longitude, age, gender }

Action:

Call latLngToZone(latitude, longitude) to compute zone.

Create a Villager document with zone populated.

Respond with 201 and { success: true, villagerId: "<auto-generated id>" }.

Validation: required fields; mobile number format basic check.

POST /api/v1/patient-requests — add_patient

Body JSON: { user_id, state, district, zone, symptoms, age, complaint, days }

Action:

Verify user_id exists.

Obtain { ph, turbidity } by calling zoneToWaterParams(zone) (if zone missing, compute from user lat/lng).

Save PatientRequest with status=1 (unsolved), ph, turbidity, and createdAt.

Return 201 and saved complaint.

POST /api/v1/dirty-water-complaints — add_dirty_watercomplaint

Body JSON: { user_id, state, district, zone, age, complaint, photo }

Action:

Validate user.

Get { ph, turbidity } from zoneToWaterParams(zone).

Save with status=1 (unsolved) and createdAt.

Return 201 and saved complaint.

POST /api/v1/workers — add_worker

Body JSON: { user_id, password, mobile_no, name, last_name, alloted_zone, age, gender }

Action:

user_id becomes _id of worker document if provided; otherwise auto-generate.

Hash password and store in passwordHash.

Save worker with status=1 (on-duty) by default.

Return 201 and worker id (do not return password hash).

PUT /api/v1/complaints/:complaintId/status — set_status (complaint)

Body JSON: { status } where status is 1/2/3.

Action:

Update the complaint (accept both patient & dirty-water complaints — search both collections by id).

After updating, fetch the userId and respond with { success: true, notifiedUserId: "<user id>" }.

Notification sending is a TODO — add a console log TODO: send push notification to user.

POST /api/v1/predictions/trigger — get prediction (manual trigger)

Body JSON: An array of zone objects:

[
  { "zone": "Z1", "symptoms": ["s1","s2"], "ph": 7.1, "turbidity": 2.3 },
  { "zone": "Z2", "symptoms": ["s3"], "ph": 6.8, "turbidity": 3.1 }
]


Action:

Send this array to callMlPredictionApi(payload) and store the full response with the zone(s) in the ZonePrediction collection (one doc per zone or a single doc containing the full payload — choose the more convenient design but document it).

Return 200 with the saved ZonePrediction(s).

Also implement an hourly cron job (node-cron) that:

For each zone present in your AshaWorker or a static list, constructs payload objects and calls the same ML API hourly.

Saves responses to ZonePrediction collection.

Make the hourly job enabled only if ENABLE_PREDICTION_JOB=true in env.

POST /api/v1/help-requests — Ask_for_help

Body JSON: { zone } (optional workerId if worker logged in)

Action: Save a HelpRequest record and return 201 with saved id.

GET /api/v1/patient-requests — get_patient_complaint

Returns all patient complaints (optionally accept query params ?zone=..&status=..&limit=..&page=..).

GET /api/v1/dirty-water-complaints — get_dirty_water_complaints

Returns all dirty water complaints (optionally accept same query params).

GET /api/v1/predictions/:zone — get_zone_prediction(zone)

Returns the most recent ZonePrediction document(s) for the requested zone.

DELETE /api/v1/workers/:workerId — delete_worker

Deletes the worker and returns 200 with { success: true }.

PUT /api/v1/workers/:workerId/zone — set_zone

Body JSON: { zone } — updates the worker's zone.

PUT /api/v1/workers/:workerId/status — set_worker_status

Body JSON: { status } where status is 0 or 1.

Updates the worker's status.

Authentication & Roles (optional, but include basic admin & worker login endpoints)

Implement POST /api/v1/auth/worker/login — accepts { user_id, password }, validates worker and returns JWT token. (If you add auth, make add/delete worker endpoints protected by an admin token; document how to create an ADMIN_TOKEN via env — or seed an admin user in README.)

If you implement auth, allow the help-requests endpoint to optionally capture workerId from bearer token.

If you choose not to implement JWT, still hash worker passwords and store them. Document the decision and how to call protected endpoints (if any).

Other implementation details & expectations

Use Mongoose for schemas and validation at the model level. Use request validation (Joi or express-validator) for inputs.

Use express-async-handler for controllers to avoid try/catch repetition.

Include an errorHandler middleware that returns consistent JSON errors.

Add a logger (use morgan for console logging) and basic rate-limiting middleware to protect endpoints.

All timestamps should be saved (createdAt) and returned in ISO format.

Use proper HTTP status codes (201 for created, 200 for success, 400 for validation error, 404 for not found, 500 for server error).

Document the exact environment variables required (at least: MONGO_URI, PORT, JWT_SECRET, ENABLE_PREDICTION_JOB, PREDICTION_API_URL).

Create vercel.json configured for API routing (use serverless-http adapter) and ensure start and vercel-build scripts are present.

Include a sample .env.example file in the output with placeholders for all env variables.

Include inline comments in each file explaining what each function does.

At the end of the generated project, include one-page README telling how to:

install dependencies,

configure .env,

run locally,

deploy to Vercel (including any Vercel-specific env vars).

Output format requirements

Produce the directory tree (top-level folders and files).

For each file show the path and the complete file contents.

Don't omit any file the app needs to run (e.g., .env.example, .gitignore).

Ensure there are no placeholder comments like // TODO: implement left in essential places — placeholder functions mentioned above may return deterministic dummy values and be clearly labelled so I can replace them later.

Do not split the response into multiple follow-ups — produce everything in this single response.

Important final note: I will provide my actual MONGO_URI later via environment variables; do not hardcode any live DB or secret. Keep all external API endpoints (prediction API) configurable via PREDICTION_API_URL. If PREDICTION_API_URL is missing, callMlPredictionApi() should return a simulated JSON response and still save it to ZonePrediction so that the rest of the system can be tested offline.

Now produce the full project as text: directory tree and every file content.